kind: CronJob
apiVersion: batch/v1
metadata:
  name: k8s-simple-backup-cron-job
spec:
  schedule: 3 0 * * *  # Default to every day at 3:00 AM UTC
  timeZone: UTC
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: k8s-simple-backup-service-account
          restartPolicy: Never
          initContainers: # These containers run sequentially before the main container(s) and only if they all succeed
            - name: k8s-simple-backup-downscale
              image: bitnami/kubectl:latest
              command:
                - /bin/sh
                - -c
                - |
                  if [ -z "$resource_kind" ] || [ -z "$resource_name" ] || [ -z "$namespace" ]; then
                    echo "Backing up this persistent volume without scaling down any resource."
                    exit 0
                  fi
                  kubectl get -n $namespace $resource_kind/$resource_name -o jsonpath='{.spec.replicas}' > /tmp/replicas
                  kubectl scale -n $namespace $resource_kind/$resource_name --replicas=0
                  ok="0"
                  for i in $(seq 1 $scale_down_timeout); do
                    if [ "$(kubectl get -n $namespace $resource_kind/$resource_name -o jsonpath='{.status.availableReplicas}')" -eq "0" ]; then
                      ok="1"
                      break
                    fi
                    echo "Waiting for $resource_kind/$resource_name to scale down to 0 available replicas..."
                    sleep 1
                  done
                  if [ "$ok" -ne "1" ]; then
                    echo "ERROR: The $resource_kind/$resource_name did not scale down to 0 replicas."
                    exit 1
                  fi
              env: # Only provide a dynamic default value
                - &env
                  name: namespace
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              envFrom: &envFrom
                - configMapRef:
                    name: k8s-simple-backup-configmap-env
                    optional: true
                - secretRef: # Can override the configMap values
                    name: k8s-simple-backup-secret-env
                    optional: true
              securityContext: &securityContext
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
                seccompProfile:
                  type: RuntimeDefault
                # Use root to ensure all files are backed up
                runAsNonRoot: true
                runAsUser: 1000
                runAsGroup: 1000
              volumeMounts: &volumeMountsKubectl
                - mountPath: /tmp
                  name: tmp

            - name: k8s-simple-backup
              image: pschiffe/borg:latest
              volumeMounts:
                - mountPath: /data
                  name: data
                  readOnly: true
                - mountPath: /tmp
                  name: tmp
              env:
                - *env
                - name: ARCHIVE
                  value: "$(namespace)-$(date +'%Y-%m-%d-%H-%M-%S')"
              envFrom: *envFrom
              securityContext: *securityContext

          containers: # At least one container is required and run last, so use it for up-scaling
            - name: k8s-simple-backup-upscale
              image: bitnami/kubectl:latest
              command:
                - /bin/sh
                - -c
                - |
                  if [ -z "$resource_kind" ] || [ -z "$resource_name" ] || [ -z "$namespace" ]; then
                    echo "Backing up this persistent volume without scaling down any resource."
                    exit 0
                  fi
                  cur_replicas=$(kubectl get -n $namespace $resource_kind/$resource_name -o jsonpath='{.spec.replicas}')
                  if [ "$cur_replicas" -ne "0" ]; then
                    echo "ERROR: The $resource_kind/$resource_name did not stay scaled down to 0 replicas."
                    echo "       Data may be corrupted!" 
                    echo "Check that your controllers do not automatically scale up the replicas while the backup is running."
                    exit 1
                  fi
                  replicas_to_restore=$(cat /tmp/replicas)
                  kubectl scale -n $namespace $resource_kind/$resource_name --replicas=$replicas_to_restore
                  ok="0"
                  for i in $(seq 1 $scale_up_timeout); do
                    if [ "$(kubectl get -n $namespace $resource_kind/$resource_name -o jsonpath='{.status.availableReplicas}')" -eq "$replicas_to_restore" ]; then
                      ok="1"
                      break
                    fi
                    echo "Waiting for $resource_kind/$resource_name to scale up to $replicas_to_restore available replicas..."
                    sleep 1
                  done
                  if [ "$ok" -ne "1" ]; then
                      echo "ERROR: The $resource_kind/$resource_name did not scale up to $replicas_to_restore replicas."
                      exit 1
                  fi
              env: [ *env ]
              envFrom: *envFrom
              volumeMounts: *volumeMountsKubectl
              securityContext: *securityContext
          volumes:
            - name: data
              persistentVolumeClaim:
                # readOnly: true  # May need to write for restore commands
                claimName: YOU_MUST_OVERRIDE_THE_claimName_VALUE  # Replace using Kustomize patches
            - name: tmp
              emptyDir: { }